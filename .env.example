# Zikos Configuration
# Copy this file to .env and adjust values for your setup

# LLM Configuration
LLM_MODEL_PATH=
# Tool format: auto (detect from model), qwen (<tool_call> JSON), simplified (<tool> key:value), native (API only)
LLM_TOOL_FORMAT=auto

# System Prompt KV Cache (optional)
# Path to pre-computed system prompt KV cache file
# If set and file doesn't exist, it will be auto-generated on startup
# Can be generated manually with: python scripts/generate_system_prompt_cache.py
SYSTEM_PROMPT_CACHE_PATH=

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=false

# Storage Paths
AUDIO_STORAGE_PATH=audio_storage
MIDI_STORAGE_PATH=midi_storage
NOTATION_STORAGE_PATH=notation_storage

# Enable verbose tool calling logs
DEBUG_TOOL_CALLS=false

# ============================================================================
# LLM Configuration
# ============================================================================
# NOTE: These variables override default constants. Don't edit those unless
# you know what you're doing

# LLM backend: auto, llama_cpp, or transformers
# auto will detect based on model file extension
LLM_BACKEND=auto

# Context window size (tokens)
# Context window size (auto-detected from model if not set)
# Recommended: 32768 for 8GB VRAM, 65536 for 16-24GB, 131072 for 40GB+
LLM_N_CTX=

# Number of GPU layers (-1=auto, 0=CPU only)
# Use 0 for CPU-only, -1 for automatic detection
LLM_N_GPU_LAYERS=-1

# Sampling temperature (0.0-1.0, higher = more creative)
LLM_TEMPERATURE=0.7

# Top-p sampling (0.0-1.0, nucleus sampling)
LLM_TOP_P=0.9
