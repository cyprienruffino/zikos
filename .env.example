# Zikos - AI Music Teacher Configuration
# Copy this file to .env and adjust values for your setup


# Path to LLM model file (required)
# For GGUF models (llama-cpp-python):
# LLM_MODEL_PATH=./models/Qwen2.5-7B-Instruct-Q4_K_M.gguf
# For Transformers models (HuggingFace):
# LLM_MODEL_PATH=./models/Qwen_Qwen3-32B-Instruct
LLM_MODEL_PATH=


# ============================================================================
# API Configuration
# ============================================================================

# API server bind address
API_HOST=0.0.0.0

# API server port
API_PORT=8000

# Enable auto-reload on code changes (development only)
API_RELOAD=false

# ============================================================================
# Storage Paths
# ============================================================================

# Directory for storing uploaded audio files
AUDIO_STORAGE_PATH=audio_storage

# Directory for storing MIDI files
MIDI_STORAGE_PATH=midi_storage

# Directory for storing notation images (sheet music, tabs)
NOTATION_STORAGE_PATH=notation_storage

# ============================================================================
# MCP Server Configuration
# ============================================================================

# MCP server host
MCP_SERVER_HOST=localhost

# MCP server port
MCP_SERVER_PORT=8001

# ============================================================================
# Debug Configuration
# ============================================================================

# Enable verbose tool calling logs
DEBUG_TOOL_CALLS=false

# ============================================================================
# LLM Configuration
# ============================================================================
# NOTE: These variables override default constants. Don't edit those unless
# you know what you're doing


# LLM backend: auto, llama_cpp, or transformers
# auto will detect based on model file extension
# LLM_BACKEND=auto

# Context window size (tokens)
# Recommended: 32768 for 8GB VRAM, 65536 for 16-24GB, 131072 for 40GB+
# LLM_N_CTX=32768

# Number of GPU layers (-1=auto, 0=CPU only)
# Use 0 for CPU-only, -1 for automatic detection
#LLM_N_GPU_LAYERS=-1

# Sampling temperature (0.0-1.0, higher = more creative)
#LLM_TEMPERATURE=0.7

# Top-p sampling (0.0-1.0, nucleus sampling)
# LLM_TOP_P=0.9
